{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrMogYNs-RbX"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pJs4iEcqyQm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "from glob import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgUuOCVB9bCc"
   },
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iYUFHV-9Lkg"
   },
   "outputs": [],
   "source": [
    "!rm -rf /usr/local/lib/python2.7\n",
    "!rm -rf /swift\n",
    "!rm -rf /tensorflow-1.15.2/python2.7\n",
    "\n",
    "!wget http://msvocds.blob.core.windows.net/coco2014/train2014.zip -O /tmp/train2014.zip\n",
    "\n",
    "!unzip -q /tmp/train2014.zip -d /tmp/data\n",
    "!rm /tmp/train2014.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubvGsbAc-Z9V"
   },
   "source": [
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsLXlnkuHj8A"
   },
   "outputs": [],
   "source": [
    "# style to be trained\n",
    "style_img = 'style1.jpg'\n",
    "\n",
    "# test image to check progress\n",
    "content_img = 'dancing.jpg'\n",
    "\n",
    "style_file_path = '.../style/'\n",
    "content_file_path = '.../content/'\n",
    "output_file_path = '.../stylized2/'\n",
    "\n",
    "# test data\n",
    "train_file_path = '/tmp/data/train2014/'\n",
    "\n",
    "saved_weights_file_path = '.../saved_weights/'\n",
    "\n",
    "\n",
    "content_layers = ['block4_conv2']\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDaM2ZGJ-u5V"
   },
   "source": [
    "# image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_CDmMDAJSR_"
   },
   "outputs": [],
   "source": [
    "# rescale style image to content dimensions\n",
    "# training dims 256 x 256 fixed\n",
    "# max dim=512\n",
    "def rescale_image(img_path, dims = None):\n",
    "  dim = 512\n",
    "  img = cv2.imread(img_path)\n",
    "  a, b, _ = img.shape\n",
    "  a, b = min(a,dim), min(b,dim)\n",
    "  if(dims != None):\n",
    "    a, b = dims[1], dims[2]\n",
    "  x = cv2.resize(img, (a, b))\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLfYzS_R8Bt_"
   },
   "outputs": [],
   "source": [
    "# scale tensor values to lie in range [0, 1]\n",
    "# this though not necessary speeds up learning\n",
    "def image_to_tensor(x):\n",
    "  x = x / 255.0\n",
    "  if(len(x.shape)<4):\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "  x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCzBEYqE3K8y"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(tensor):\n",
    "  x = np.array(tensor) * 255.0\n",
    "  x = x[0]\n",
    "  x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "\n",
    "  plt.imshow(x)\n",
    "  plt.show()\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9N3m9D24-6A0"
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJw0qqVrGjHo"
   },
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "  vgg = VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "  outputs = []\n",
    "\n",
    "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "  \n",
    "  model = tf.keras.Model([vgg.input], outputs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0FniBcCk1-L"
   },
   "outputs": [],
   "source": [
    "# instance normalization normalizes across the channels\n",
    "# it tries to address that network should be unbiased towards contrast of the image\n",
    "# works better than batch normalization\n",
    "\n",
    "# conv -> instanceNorm -> activation\n",
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, filters, \n",
    "               kernel=(3,3), padding='same', \n",
    "               strides=(1,1), activate=True, name=\"\", \n",
    "               weight_initializer=\"glorot_uniform\"\n",
    "               ):\n",
    "    super(ConvLayer, self).__init__()\n",
    "    self.activate = activate\n",
    "    self.conv = tf.keras.layers.Conv2D(filters, kernel_size=kernel, \n",
    "                       padding=padding, strides=strides, \n",
    "                       name=name, trainable=True,\n",
    "                       use_bias=False, \n",
    "                       kernel_initializer=weight_initializer)\n",
    "    self.inst_norm = tfa.layers.InstanceNormalization(axis=3, \n",
    "                                          center=True, \n",
    "                                          scale=True, \n",
    "                                          beta_initializer=\"zeros\", \n",
    "                                          gamma_initializer=\"ones\",\n",
    "                                          trainable=True)\n",
    "    if self.activate:\n",
    "      self.relu_layer = tf.keras.layers.Activation('relu')\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = self.inst_norm(x)\n",
    "    if self.activate:\n",
    "      x = self.relu_layer(x)\n",
    "    return x\n",
    "\n",
    "# convT -> instanceNorm -> activation\n",
    "class ConvTLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel=(3,3), padding='same',\n",
    "               strides=(1,1), activate=True, name=\"\",\n",
    "               weight_initializer=\"glorot_uniform\" \n",
    "               ):\n",
    "    super(ConvTLayer, self).__init__()\n",
    "    self.activate = activate\n",
    "    self.conv_t = tf.keras.layers.Conv2DTranspose(filters, kernel_size=kernel, padding=padding, \n",
    "                                  strides=strides, name=name, \n",
    "                                  use_bias=False,\n",
    "                                  kernel_initializer=weight_initializer)\n",
    "    self.inst_norm = tfa.layers.InstanceNormalization(axis=3, \n",
    "                                          center=True, \n",
    "                                          scale=True, \n",
    "                                          beta_initializer=\"zeros\", \n",
    "                                          gamma_initializer=\"ones\",\n",
    "                                          trainable=True)\n",
    "    if self.activate:\n",
    "      self.relu_layer = tf.keras.layers.Activation('relu')\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv_t(x)\n",
    "    x = self.inst_norm(x)\n",
    "    if self.activate:\n",
    "      x = self.relu_layer(x)\n",
    "    return x\n",
    "\n",
    "# (conv1 -> conv2) + x\n",
    "class ResBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel=(3,3), padding='same', weight_initializer=\"glorot_uniform\", prefix=\"\"):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.prefix_name = prefix + \"_\"\n",
    "    self.conv1 = ConvLayer(filters=filters, \n",
    "                           kernel=kernel, \n",
    "                           padding=padding, \n",
    "                           weight_initializer=weight_initializer,\n",
    "                           name=self.prefix_name + \"conv_1\")\n",
    "    self.conv2 = ConvLayer(filters=filters, \n",
    "                           kernel=kernel, \n",
    "                           padding=padding, \n",
    "                           activate=False, \n",
    "                           weight_initializer=weight_initializer,\n",
    "                           name=self.prefix_name + \"conv_2\")\n",
    "    self.add = tf.keras.layers.Add(name=self.prefix_name + \"add\")\n",
    "\n",
    "  def call(self, x):\n",
    "    tmp = self.conv1(x)\n",
    "    c = self.conv2(tmp)\n",
    "    return self.add([x, c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0rm-0o2YGLN"
   },
   "outputs": [],
   "source": [
    "class TransformNet(tf.keras.models.Model):\n",
    "  def __init__(self):\n",
    "    super(TransformNet, self).__init__()\n",
    "    self.conv1 = ConvLayer(32, (9,9), strides=(1,1), padding='same', name=\"conv_1\")\n",
    "    self.conv2 = ConvLayer(64, (3,3), strides=(2,2), padding='same', name=\"conv_2\")\n",
    "    self.conv3 = ConvLayer(128, (3,3), strides=(2,2), padding='same', name=\"conv_3\")\n",
    "    self.res1 = ResBlock(128, prefix=\"res_1\")\n",
    "    self.res2 = ResBlock(128, prefix=\"res_2\")\n",
    "    self.res3 = ResBlock(128, prefix=\"res_3\")\n",
    "    self.res4 = ResBlock(128, prefix=\"res_4\")\n",
    "    self.res5 = ResBlock(128, prefix=\"res_5\")\n",
    "    self.convt1 = ConvTLayer(64, (3,3), strides=(2,2), padding='same', name=\"conv_t_1\")\n",
    "    self.convt2 = ConvTLayer(32, (3,3), strides=(2,2), padding='same', name=\"conv_t_2\")\n",
    "    self.conv4 = ConvLayer(3, (9,9), strides=(1,1), padding='same', activate=False, name=\"conv_4\")\n",
    "    self.tanh = tf.keras.layers.Activation('tanh')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # inputs in form of (None, None, None, 3) tensor\n",
    "    x = self.conv1(inputs)\n",
    "    x = self.conv2(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.res1(x)\n",
    "    x = self.res2(x)\n",
    "    x = self.res3(x)\n",
    "    x = self.res4(x)\n",
    "    x = self.res5(x)\n",
    "    x = self.convt1(x)\n",
    "    x = self.convt2(x)\n",
    "    x = self.conv4(x)\n",
    "    x = self.tanh(x)\n",
    "    x = (x + 1) / 2\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1UCtnSD0wwZ1"
   },
   "outputs": [],
   "source": [
    "class ModelClass(tf.keras.models.Model):\n",
    "  def __init__(self, all_layers):\n",
    "    super(ModelClass, self).__init__()\n",
    "    self.vgg =  vgg_layers(all_layers)\n",
    "    self.all_layers = all_layers\n",
    "    self.vgg.trainable = False\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # inputs in form of (None, None, None, 3) tensor\n",
    "    # preprocessing is a part of the model\n",
    "    inputs = inputs * 255.0\n",
    "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "    outputs = self.vgg(preprocessed_input)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0SZGN0y_FXv"
   },
   "source": [
    "# losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCgadEvgA9do"
   },
   "outputs": [],
   "source": [
    "def compute_content_cost(a_C, a_G):\n",
    "  m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "  J_content = 0\n",
    "  for i in range(m):\n",
    "    J_content = J_content + tf.reduce_sum(tf.square(tf.subtract(a_C[i,:,:,:],a_G[i,:,:,:])))/(4*n_C*n_H*n_W)\n",
    "\n",
    "  J_content = J_content / m\n",
    "  return J_content\n",
    "\n",
    "def variational_loss(A):\n",
    "  m, H, W, C = A.get_shape().as_list()\n",
    "  return tf.reduce_sum(tf.image.total_variation(A)) / (m*W*H)\n",
    "\n",
    "def gram_matrix(A):\n",
    "  GA = tf.matmul(A,tf.transpose(A,[1,0]))\n",
    "\n",
    "  return GA\n",
    "\n",
    "def compute_layer_style_cost(a_S, a_G):\n",
    "  J_style_layer = 0\n",
    "  m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "  for i in range(m):\n",
    "    a_Sw = tf.transpose(tf.reshape(a_S[i,:,:,:], [n_H*n_W, n_C]), [1,0])\n",
    "    a_Gw = tf.transpose(tf.reshape(a_G[i,:,:,:], [n_H*n_W, n_C]), [1,0])\n",
    "    GS = gram_matrix(a_Sw)\n",
    "    GG = gram_matrix(a_Gw)\n",
    "    J_style_layer = J_style_layer + tf.reduce_sum(tf.square(tf.subtract(GS,GG)))/(4*(n_C*n_H*n_W)**2)\n",
    "\n",
    "  J_style_layer = J_style_layer / m\n",
    "  return J_style_layer\n",
    "\n",
    "def total_cost(content_outputs, style_outputs, outputs, transformed, alpha = 100000, beta = 40, gamma = 20):\n",
    "  content = compute_content_cost(content_outputs, outputs[0])\n",
    "  variational = variational_loss(transformed)\n",
    "  style = 0\n",
    "\n",
    "  for i in range(1,len(outputs)):\n",
    "    style = style + 0.2*compute_layer_style_cost(style_outputs[i-1], outputs[i])\n",
    "  \n",
    "  return beta*style + content*alpha + gamma*variational\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-dLZo4RIll5"
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4B1NEHauWfT"
   },
   "outputs": [],
   "source": [
    "style_extractor = ModelClass(style_layers)\n",
    "content_extractor = ModelClass(content_layers)\n",
    "extractor = ModelClass(content_layers + style_layers)\n",
    "\n",
    "# actual model\n",
    "model = TransformNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mquX-EWGH39g"
   },
   "outputs": [],
   "source": [
    "def StyleTransfer(style_img, style_path, train_file_path):\n",
    "\n",
    "  optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(content_tensor, style_tensor):\n",
    "    with tf.device('/GPU:0'):\n",
    "      content_outputs = content_extractor(content_tensor)\n",
    "      style_outputs = style_extractor(style_tensor)\n",
    "      with tf.GradientTape() as tape:\n",
    "        x = model(content_tensor)\n",
    "        outputs = extractor(x)\n",
    "        loss = total_cost(content_outputs, style_outputs, outputs, x)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    " \n",
    "  epochs = 2\n",
    "  minibatch_size = 10\n",
    "  step = 0\n",
    "\n",
    "  check = 100\n",
    "  save_check = 1000\n",
    "\n",
    "  loss = []\n",
    "\n",
    "  train_images = glob(os.path.join(train_file_path, \"*.jpg\"))\n",
    "  num_images = len(train_images) - (len(train_images) % minibatch_size)\n",
    "\n",
    "  print(num_images, 'training images')\n",
    "\n",
    "  for n in range(epochs):\n",
    "    for i in range(0, num_images, minibatch_size):\n",
    "      content_imgs = [rescale_image(img_path, dims=[1, 256, 256]) for img_path in train_images[i:i+minibatch_size]]\n",
    "      style_imgs = [rescale_image(style_path, dims=[1, 256, 256]) for j in range(len(content_imgs))]\n",
    "      content_imgs = np.array(content_imgs)\n",
    "      style_imgs = np.array(style_imgs)\n",
    "\n",
    "      content_tensor = image_to_tensor(content_imgs)\n",
    "      style_tensor = image_to_tensor(style_imgs)\n",
    "\n",
    "      loss_ = train_step(content_tensor, style_tensor)\n",
    "\n",
    "      if(step%check == 0):\n",
    "        print(i, 'images processed')\n",
    "        loss.append(loss_)\n",
    "        \n",
    "      if(step%save_check == 0):\n",
    "        content_path = content_file_path + content_img\n",
    "        test_image = rescale_image(content_path)\n",
    "        test_image = image_to_tensor(test_image)\n",
    "\n",
    "        x = model(test_image)\n",
    "        x = deprocess_image(x)\n",
    "        model.save_weights(saved_weights_file_path + str(style_img[:-4]))\n",
    "        print('checkpoint... model saved')\n",
    "\n",
    "      step = step + 1\n",
    "    step = 0\n",
    "    print(\"epochs : \" + str(n))\n",
    "\n",
    "\n",
    "  loss = np.array(loss)\n",
    "  plt.plot(loss, label = 'loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  model.save_weights(saved_weights_file_path + str(style_img[:-4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lb7LdInIsxp"
   },
   "source": [
    "# execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "16bGTBBID2wGdSzihF8ITt2qcxNulxRDv"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1276128,
     "status": "ok",
     "timestamp": 1591351460310,
     "user": {
      "displayName": "Ranajay Medya",
      "photoUrl": "",
      "userId": "02160689287509861187"
     },
     "user_tz": -330
    },
    "id": "ZxTfjufhOu1s",
    "outputId": "bae7185f-ca3c-4ac1-a2fd-8103353739b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "style_path = style_file_path + style_img\n",
    "# use of colab gpu\n",
    "# training\n",
    "StyleTransfer(style_img, style_path, train_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0I5KpKg01SZ"
   },
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9Yg_wJq8iBR"
   },
   "outputs": [],
   "source": [
    "# trained style\n",
    "style_img = 'style1.jpg'\n",
    "# image to be transformed\n",
    "content_img = 'dancing.jpg'\n",
    "\n",
    "new_model = TransformNet()\n",
    "new_model.load_weights(saved_weights_file_path + str(style_img[:-4]))\n",
    "\n",
    "content_path = content_file_path + content_img\n",
    "test_image = rescale_image(content_path)\n",
    "test_image = image_to_tensor(test_image)\n",
    "\n",
    "x = new_model(test_image)\n",
    "x = deprocess_image(x)\n",
    "\n",
    "morphed_img = content_img[:-4] + '_' + style_img\n",
    "output_path = output_file_path + morphed_img\n",
    "cv2.imwrite(output_path, x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO8LCF+F03iRBcz3FXPGt9j",
   "collapsed_sections": [],
   "mount_file_id": "1koFa1kNDjAxDqMzAFFWDYssLw574mwZK",
   "name": "Fast_NStyle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
